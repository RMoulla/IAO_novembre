{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/IAO_novembre/blob/main/Insign_Fine_tuning_Mistral_7B_WritingPrompts_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYvJNNYLDR4"
      },
      "source": [
        "# **Fine-tuning d'un modèle Mistral 7B avec LoRA sur WritingPrompts**\n",
        "\n",
        "## Objectif du TP\n",
        "\n",
        "Ce TP vise à vous familiariser avec le fine-tuning d'un modèle de langage de grande taille (*Large Language Model*, LLM) en utilisant une approche efficace en termes de mémoire : **LoRA** (*Low-Rank Adaptation*). Vous fine-tunerez un modèle **Mistral 7B** sur un sous-ensemble du dataset **WritingPrompts**, un corpus conçu pour entraîner des modèles à générer du texte créatif à partir de consignes d’écriture.\n",
        "\n",
        "À l’issue de ce TP, vous serez capable de :\n",
        "- Comprendre le principe de l’adaptation des grands modèles via LoRA.\n",
        "- Charger et prétraiter un dataset textuel pour le fine-tuning.\n",
        "- Configurer et exécuter un entraînement LoRA en utilisant la bibliothèque `peft` de Hugging Face.\n",
        "- Évaluer les performances du modèle après fine-tuning.\n",
        "- Générer du texte à partir d’un modèle fine-tuné.\n",
        "\n",
        "\n",
        "\n",
        "## Plan du TP\n",
        "\n",
        "Le TP se décline selon les étapes suivantes :\n",
        "\n",
        "1. **Préparation des données**\n",
        "   - Chargement des données d'entrainement, de validation et de test.\n",
        "\n",
        "2. **Préparation de l'environnement et du modèle Mistral 7B**  \n",
        "   - Configuration de l'environnement.\n",
        "   - Chargement du modèle pré-entraîné.  \n",
        "   - Configuration des poids LoRA.  \n",
        "\n",
        "3. **Fine-tuning du modèle**  \n",
        "   - Définition des hyperparamètres et de l'entraînement.  \n",
        "   - Lancement du fine-tuning avec `Trainer` de Hugging Face.  \n",
        "\n",
        "4. **Évaluation et génération de texte**  \n",
        "   - Comparaison avant/après fine-tuning.  \n",
        "   - Tests sur de nouvelles consignes d’écriture.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmX_2mCou-A-",
        "outputId": "72f24f59-99b0-4ee7-8ffa-ea9a90dc4c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.8.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=792209179c05dc5f77df240af875cb82659050a8ec61e7f0d70c4e63e6c0923c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] datasets accelerate tqdm rouge-score\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install -q bitsandbytes trl peft tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQVkLa4FDE3x"
      },
      "source": [
        "## Préparation des données :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpNzN1_MMABn"
      },
      "source": [
        "Installation des dépendances nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4Tio8agMVwq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlIroXy7iHJZ"
      },
      "source": [
        "Chargement des donnéees d'entraînement, de validation et de test. Les données ont déjà été préalabelement pré-traités (suppression des données non pertinentes, des balises HTML, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IzikyFBL1sV"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train_df.csv')\n",
        "validation_df = pd.read_csv('validation_df.csv')\n",
        "test_df = pd.read_csv('test_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOXO9md_16f0",
        "outputId": "3ac0c9e4-43c5-4f0d-8bad-1792ce0980f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4000, 9), (1000, 3), (1000, 3))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape, validation_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ah4G_KMIJ7",
        "outputId": "0a8dbd17-afb0-475c-c1e6-f9c792f4b7d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ WP ] You 've finally managed to discover the secret to immortality . Suddenly , Death appears before you , hands you a business card , and says , `` When you realize living forever sucks , call this number , I 've got a job offer for you . ''\n",
            "\n",
            "---\n",
            "So many times have I walked on ruins, the remainings of places that I loved and got used to.. At first I was scared, each time I could feel my city, my current generation collapse, break into the black hole that thrives within it, I could feel humanity, the way I'm able to feel my body.. After a few hundred years, the pattern became obvious, no longer the war and damage that would devastate me over and over again in the far past was effecting me so dominantly. \n",
            " It's funny, but I felt as if after gaining what I desired so long, what I have lived for my entire life, only then, when I achieved immortality I started truly aging. \n",
            " \n",
            " 5 world wars have passed, and now they feel like a simple sickeness that would pass by every so often, I could no longer evaluate the individual human as a being of its own, the importance of mortals is merely the same as the importance of my skin cells; They are a part of a mechanism so much more advanced, a mechanism that is so dear to my fallen heart a mechanism that I have seen fall and rise so many times, a mechanism that when lost all of which it had, had me loosing my will to live, for the first time in all of my thousands years of existence. \n",
            " \n",
            " Acceptance, something so important. a skill that has proved itself worthy dozens of times, an ability that looks so easy to achieve, a gift, that I was n't able to aquire in all my years, until now. When the ashes on the ground flew into the now empty air upon humanity's fall, I felt as if all of it's weight was crushing me. Ignorance took over and I searched years for a hope, a sign of the very same patterns that I used to watch reappear every hundred years, the very core of my will to exist that was now no more that I so strongly wish was. \n",
            " \n",
            " If you have ever wondered if silence can drive people crazy, it can.. \n",
            " I ca n't feel my legs, I have walked for days, just to hear the sound of gravel, crushed bones, crushed buildings and crushed civilizations under my steps to keep my sanity.. until I remembered, the day in my far past. The day of my rebirth, I took out of my pocket a small plastic box, with nine buttons and a small glass window. I could n't believe this was our past, I could n't believe how far we have been able to progress and yet, be destroyed by our own violence. \n",
            " I slowly dialed the number I was given, exactly 1729 years ago. \n",
            " \n",
            " I dropped a tear, a tear that was too slow to hit the ground as I got sucked into the darkness that emerged around me. \n",
            " \n",
            " A chill went through my spine as I saw my destiny rise above me, I could see the white teeth under the dark cloack... \n",
            " \n",
            " `` You have finally arrived'' He projected into my mind, with the most chilling cold and unhuman voice. \n",
            " \n",
            " `` I'm ready to obey'' I answered. I knew who was sitting infront of me, and it was time for me to obey him, after all these years of playing god, even I came to it. \n",
            " \n",
            " Funny is n't it? Even by achieving immortality, death, is inescapable.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_df.loc[0]['prompt'])\n",
        "print('---')\n",
        "print(train_df.loc[0]['story'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2c2NpyHUqkI"
      },
      "source": [
        "## Préparation de l'environnement et du modèle Mistral 7B\n",
        "\n",
        "Configuration de l'utilisation du GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DQwcp3_6teq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hntRTROyU_3Y"
      },
      "source": [
        "Authentification avec Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqsE-DTXz8aj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "HF_KEY = \"\"\n",
        "login(HF_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKnAJAGRVSYJ"
      },
      "source": [
        "Chargement du modèle et du tokenizer et configuration de la quantisation sur 5 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ljlm7KyyTTn"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA6GAwkg6hIH"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544,
          "referenced_widgets": [
            "de5fb70cac4d4d32b7c060d215e21416",
            "3a5d44e19eb54b86ba722365d76281ff",
            "08abf2859b524380b139671449f0c352",
            "d47d78ccaeb2449e84cd11c5c58241de",
            "ad4fea588ec84c94a8fd8de954dce3fd",
            "330ad2e685f744869fb547a6c710e29b",
            "45b327cc15a44ce1891d2c7981071942",
            "aa8233400112427b8a38a69547b8e0b6",
            "ea6eb0cf246440f0b77ed1b2b62e4cce",
            "84cc2c4218e1460f8a501930c67316bd",
            "3041ead8c0e44d53a34384bc95bda028",
            "b64bd84eac154d69bfd2b2ec92ea66b9",
            "1a2c40ee3f13458a8a0cb355cb41044c",
            "356a3cce30ff4f21879e9ae37235a8fd",
            "7491b6eb1b47440ea93d996b8f917e58",
            "a305bcedd1454b45835dd1d9a3584090",
            "a70080cc06224089bc76decaa08d2687",
            "0f7369aa81474e308283d23938c3191c",
            "ad4539f745e641a1945c1fb18055f196",
            "4b0ca7747f084c6bb870d80391d720b8",
            "f5a3c25e1d6d442eab0b201082873d3c",
            "01c4eb8361c842299261421f40c9e5c4",
            "fcd4e40fe9f94e03abfe2c3aa44c1001",
            "ca94ea8ec0ee47b7882e5997e09540f6",
            "c80be897d34a441ebc19bc6ef1b41d34",
            "9a33aa517a794888817fa78d6abab029",
            "aaa31dabfd964639a0b2ee2998d58b9c",
            "b555c5ddd6464e4db2e3a1a8a0f2b2a4",
            "901c5f1bf5d14f579999a53b32e7aa3e",
            "7ba6dde48dd044d792d247d6c6d5f4bc",
            "93049dde37ef49a4bdf817849e291ff6",
            "127f93c4c2e44994b42b162e92834710",
            "feab4f37514140608d899f46e24bfc41",
            "a56fd9cf57cd4d7c960aea87e3515fad",
            "9ef89e6e49cc4d9b84827aa77d1e661e",
            "7792ad461ba346cb8821a08c741516ea",
            "4e8342b1a6894a059ea9f384931d2539",
            "ecbb3450f0cd470ea1953ad27e645025",
            "727f0870941048f7b7730492819ffce9",
            "fbc01c10a89c41258f7f606dd99188df",
            "aa982ad2eaa14300a87e5d01c337e571",
            "9bf3d0b4a88d4ec7a4a716faaea8cdb2",
            "770a4b45f4074142a459275f0ea4e845",
            "12b24d38a3dd4158af4b6b298a3d1c45",
            "9fe2762de90a49ccae9aa4a7edea3033",
            "2ab8b9fde2ec478887dd9066b78a271e",
            "10901b247ba546378bd95ece67b38361",
            "7978d477b08a4bf19c23a3f39e851b6c",
            "51894bc8966a4735801acd6cbc808b0b",
            "98a34159d6bf410592c479298a3c0b87",
            "5464340f3a5547e0b70511313240b972",
            "5488e0312aff4371ae9f172882f4a7b7",
            "879978de7830490497e22094a019305a",
            "220debd600004902ac57aeab4d819783",
            "2ce9cd25eef3409cb05b91681efa5274",
            "9c0007c8e1c04622921bdba756456571",
            "1a4eb2f9f9f549f38a868edeb332e432",
            "edbf730d8cfe425cac358a9b72f956f7",
            "c8ec934aaaf2403f9b52207947410e87",
            "d95bc81edb0b46e79f14371c2b93d576",
            "2a1413f91ff04deea4faade59c57a8c7",
            "91dbda3237294deabb0a0cb8a7280ffc",
            "cf64cc36b6dc4ddbafedc77510bb9f88",
            "90579c9752de4a68bce7959cfe13dc41",
            "256de53e436e4251a6098db9cfb577b2",
            "fb0055a17a74414d80a39c79227cbc1a",
            "53ede110339e422babb42479624c9632",
            "e74f01cd02b04dc0a9a020c37a6d788b",
            "f8b1e555f36d4f9c8560e1b2df883bcf",
            "9f6e4235b7d24d2498d877369c264ba0",
            "cf1045ec5ad041249e4ee5e644227834",
            "f43f9c41dede446e9bd9c7075e4e96bb",
            "a5cfd9da1a3b486cbe1f69908ef0842b",
            "79cdecbe8f354d6580916e617237b746",
            "0318d1c7f59e41089545aba054194bf1",
            "06d17ca787704c37add052e48acf791b",
            "b1073612b9e6448b88c3e91732def495",
            "6a5aedf038e14eef8910b4c908a334de",
            "bc382504cadd4df98741ed42369a195a",
            "a087d3659dbe4fe4abba56acf713ad38",
            "fdba58fa65a64f79b9c9b62da15f1933",
            "12ce4ed632fa406f9d56322f777ac56e",
            "98798e7da1b544f49b0009f57261dde2",
            "55b516bb601d40fdb39752c5b9a86937",
            "46de9b0b94d843b98ac06fd5a5a85bf5",
            "b1155c741c744c50ba0bbfd8c5b046a7",
            "e3686725831b4cf2acdfd04bc41bfd30",
            "da80cd0c1a5c4829b133700c403d1680",
            "c089df13d19b422aa350d35eb500aa58",
            "1c5fa37ce2c340cdbe6ca8f37b2d4de7",
            "a7a0d712be804532b5c4836478d4edb8",
            "72b518b44ee14d23bab2e0797fb236ce",
            "06c7476e443d4cc3827de543faae9faf",
            "2b83765e9ba849d885292a91076fa247",
            "50f44508053a4a8a87f8b9a15b9e35cd",
            "9454afeaf15d46c7a2533abc323ce318",
            "7fa00c2526eb44998a9de0289fa11163",
            "8ff50500b9a045c7b365201c126d4a56",
            "9bffbc60874c43499a57477f63c42a37",
            "c1723fd473904985bf0482daf3c23dfe",
            "b87615b62ae340feabf2163e46561ac9",
            "b70368d66cce4a06b9eb6be05ab537bf",
            "22b757ddf5af450b892a5765715fc015",
            "71b28ed6b3ba4f87aa6c4a3e5e63b888",
            "071deb0cae7f4ab4b7e873e4e51fe5dd",
            "ca4f3e09bbf04d4c8d50e3961732e0fd",
            "7e65312ef97b40cd99ae4c64c8f77d13",
            "7a92814ddaf64d0e879af857b1b7d212",
            "2dc7a0ffe34243aaaecf8ff177471648",
            "15b8505a6b61410ba323c095324aefb6",
            "bb9a0952f56c4821abd8e594b19b55fd",
            "de9b15634fe848cb8e720c379ed071b7",
            "28ae3669288044039ea75705cd703cdd",
            "02058b6c52c64487b1dd6bce20061869",
            "eefd1d0d5f0743db9af261c748d7da03",
            "e5d6a410d4fb45a4bb76582306b80de4",
            "3f0ae3d3e72b4f32887adea1ce0efa03",
            "485bfdb516964d359c497c6ed7782c2f",
            "dbf0113e58dd48218fc7bb1bedd17852",
            "7a8c8af03f494d4194671969c2ddb198",
            "8d35634a4a3f4390afbd7aac78a768ff",
            "4e83a4f19faf43dbb0c4500a59888f12",
            "65a6aa609b0f45cd91cc25a4fca30481",
            "05af6224c04b4c79b2724044ffd16a33",
            "99be27ca93934293bf2db8e074da881a",
            "41637edaeb1c4c6892bfb8125fb4edf0",
            "1f7a8891cb5b4bb79b49637518e01c26",
            "098a8786958f44ccad30addb7015d245",
            "40080de69a5b4c91b9188fcf011012c6",
            "ef77c6bae5aa446e9115ca8cbd2dff68",
            "6c191afe3eba4e1cb6a57ec6d13c30f9",
            "d3871dea37dc423fb0db0834a4c724f7"
          ]
        },
        "id": "vZNQuwq56TwJ",
        "outputId": "fbc58a61-0e66-4d0d-e73e-68b6ab12c742"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de5fb70cac4d4d32b7c060d215e21416",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b64bd84eac154d69bfd2b2ec92ea66b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcd4e40fe9f94e03abfe2c3aa44c1001",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a56fd9cf57cd4d7c960aea87e3515fad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fe2762de90a49ccae9aa4a7edea3033",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c0007c8e1c04622921bdba756456571",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53ede110339e422babb42479624c9632",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a5aedf038e14eef8910b4c908a334de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c089df13d19b422aa350d35eb500aa58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1723fd473904985bf0482daf3c23dfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9a0952f56c4821abd8e594b19b55fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e83a4f19faf43dbb0c4500a59888f12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\", attn_implementation=\"flash_attention_2\", torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Xb6GuPMKr6"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey88HUxO__zd"
      },
      "source": [
        "Configuration de LoRA pour éviter d'entraîner l'ensemble des 7 milliards de paramètres de Mistral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0lwxONc7VCa"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=4,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caslWNS28Fv1",
        "outputId": "2bf03ed8-3d94-444e-84c2-19b00f6ddaf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 23,068,672 || all params: 7,271,092,224 || trainable%: 0.3173\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARancb4XD78n"
      },
      "source": [
        "Avant de commencer le fine-tuning, nous devons configurer notre environnement pour **optimiser l'utilisation du GPU** et assurer un entraînement fluide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-Mos3qDZsK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from accelerate import Accelerator\n",
        "#Configuration des variables d'environnement pour optimiser l'efficacité GPU\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
        "\n",
        "# Initialisation de l'accélérateur\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6QmSE6lEJ1g"
      },
      "source": [
        "Une fois les données textuelles prétraitées et tokenisées, nous devons les organiser sous une forme adaptée à l'entraînement du modèle. Pour cela, nous utilisons un **data collator**, qui permet de gérer le **padding** et le **batching** des échantillons efficacement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFHpp4YEDaEI"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqOCWkP_C-9P"
      },
      "source": [
        "Gestion et optimisation de la mémoire GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XUaWWNVC9OT",
        "outputId": "42525cf7-03c0-4dda-b16e-7186dccad090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Oct  9 13:04:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   32C    P0             60W /  400W |    7453MiB /  81920MiB |      1%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 1: `kill -9 <pid>'\n",
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi\n",
        "!kill -9 <pid>\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJN5CsOjWfZ9"
      },
      "source": [
        "Séparation des données en ensembles d'entrée (X) et de sortie (y) pour l'entraînement, la validation et les tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bei-PRVwJ2_R"
      },
      "outputs": [],
      "source": [
        "X_train_data = train_df['prompt_cleaned'].values\n",
        "X_validation_data = validation_df['prompt_cleaned'].values\n",
        "X_test_data = test_df['prompt_cleaned'].values\n",
        "\n",
        "y_train_data = train_df['story'].values\n",
        "y_validation_data = validation_df['story'].values\n",
        "y_test_data = test_df['story'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxaCLtEVGnrC",
        "outputId": "6c77b253-756f-4f57-e7df-f20d1ac013a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4000,), (1000,), (1000,))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_data.shape, X_validation_data.shape, X_test_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9d1P_j8Wv3x"
      },
      "source": [
        "Tokenisation des données d'entrée (prompts) et de sortie (story) pour l'entraînement, la validation et les tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXyfa0JwJok4"
      },
      "outputs": [],
      "source": [
        "# Tokenisation des entrées\n",
        "X_train_tokens = tokenizer(X_train_data.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=400)\n",
        "X_validation_tokens = tokenizer(X_validation_data.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=400)\n",
        "X_test_tokens = tokenizer(X_test_data.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=400)\n",
        "\n",
        "# Tokenisation des sortie\n",
        "y_train_tokens = tokenizer(y_train_data.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=400)\n",
        "y_validation_tokens = tokenizer(y_validation_data.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=400)\n",
        "y_test_tokens = tokenizer(y_test_data.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=400)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GShnZuK7XCqC"
      },
      "source": [
        "Adaptation des datasets d'entraînement, de validation et de test pour le fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en3CWFo_7hPg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Création d'objets dataset\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": X_train_tokens['input_ids'],\n",
        "    \"attention_mask\": X_train_tokens['attention_mask'],\n",
        "    \"labels\": y_train_tokens['input_ids'],\n",
        "}).with_format(\"torch\")\n",
        "\n",
        "validation_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": X_validation_tokens['input_ids'],\n",
        "    \"attention_mask\": X_validation_tokens['attention_mask'],\n",
        "    \"labels\": y_validation_tokens['input_ids'],\n",
        "}).with_format(\"torch\")\n",
        "\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": X_test_tokens['input_ids'],\n",
        "    \"attention_mask\": X_test_tokens['attention_mask'],\n",
        "    \"labels\": y_test_tokens['input_ids'],\n",
        "}).with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km5bO80MEZfK"
      },
      "source": [
        "## Finte-tuning du modèle Mistral 7B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J3ch1jzZiSI"
      },
      "source": [
        "Avant de lancer le fine-tuning, nous devons définir les arguments d’entraînement qui vont déterminer le comportement du modèle pendant l’apprentissage. Nous utilisons ici la classe `TrainingArguments` de `transformers` pour configurer ces paramètres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vwdU0KQDBCy",
        "outputId": "cffb0ea5-b82b-4597-a58d-f6ce22acc103"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "output_dir = \"data/mistral-7b-sft-lora_v0.1\"\n",
        "\n",
        "# Arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    fp16=True,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5SROGjpZ7E7"
      },
      "source": [
        "Après le fine-tuning, il est essentiel d’évaluer la qualité des textes générés par le modèle. Pour cela, nous mettons en place une **fonction d’évaluation** qui calcule plusieurs métriques de performance basées sur la similarité entre le texte généré et la vérité terrain.\n",
        "\n",
        "1. **Calcul des scores ROUGE**  \n",
        "   - ROUGE (*Recall-Oriented Understudy for Gisting Evaluation*) est une famille de métriques utilisée pour comparer un texte généré à une référence.\n",
        "   - Nous utilisons `rouge1`, `rouge2` et `rougeL` avec `RougeScorer` :\n",
        "     - **ROUGE-1** : Correspond aux **unigrammes** communs entre le texte généré et la référence.\n",
        "     - **ROUGE-2** : Évalue la correspondance des **bigrams**.\n",
        "     - **ROUGE-L** : Se base sur la plus longue sous-séquence commune (*Longest Common Subsequence*).\n",
        "\n",
        "2. **Calcul de la similarité cosinus avec TF-IDF**  \n",
        "   - Nous utilisons `TfidfVectorizer` pour transformer les textes en vecteurs pondérés.\n",
        "   - La **similarité cosinus** est ensuite calculée entre les textes générés et les textes de référence.\n",
        "   - Cela permet d’avoir une évaluation plus fine en mesurant la proximité des représentations textuelles.\n",
        "\n",
        "3. **Filtrage des labels**  \n",
        "   - Certains ID de token peuvent être en dehors du vocabulaire du modèle.\n",
        "   - Nous filtrons donc ces IDs avant de décoder les labels en texte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YmUvyYFcbwV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "\n",
        "    # Convertir les prédictions et les labels en texte\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Filtrer les labels pour exclure les IDs hors du vocabulaire\n",
        "    filtered_labels = [\n",
        "        [token_id for token_id in l if 0 <= token_id < tokenizer.vocab_size]\n",
        "        for l in labels\n",
        "    ]\n",
        "    decoded_labels = tokenizer.batch_decode(filtered_labels, skip_special_tokens=True)\n",
        "\n",
        "    # Calculer les scores ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores = [scorer.score(pred, label) for pred, label in zip(decoded_preds, decoded_labels)]\n",
        "\n",
        "    rouge1 = sum(score[\"rouge1\"].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "    rouge2 = sum(score[\"rouge2\"].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "    rougeL = sum(score[\"rougeL\"].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "    # Calculer la similarité cosinus\n",
        "    vectorizer = TfidfVectorizer().fit(decoded_preds + decoded_labels)\n",
        "    tfidf_preds = vectorizer.transform(decoded_preds)\n",
        "    tfidf_labels = vectorizer.transform(decoded_labels)\n",
        "    similarities = [cosine_similarity(tfidf_preds[i], tfidf_labels[i])[0][0] for i in range(len(decoded_preds))]\n",
        "    avg_similarity = sum(similarities) / len(similarities)\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": rouge1,\n",
        "        \"rouge2\": rouge2,\n",
        "        \"rougeL\": rougeL,\n",
        "        \"similarity\": avg_similarity,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPAaqnqjaAws"
      },
      "source": [
        "Maintenant que nous avons configuré l’environnement, préparé les données, défini les paramètres d’entraînement et mis en place une fonction d’évaluation, nous pouvons initialiser le Trainer qui va gérer le fine-tuning du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "efb5ba700c564ecf98cbf643d3fd03a4",
            "5be1f8171f9447898518b95d7a4b50bf",
            "14dc9d22f8bb4b87b16188314cccf3ad",
            "7c49ffcab283412d842ebda50c77d38d",
            "d1b94ba264b143d989026ac19e545cea",
            "e1195d6a63a8449c8aa4599433b76cea",
            "aabc90a10c7d4b9f8cb89b2f7e3e4310",
            "a193b5cb30c54711b947b5911fdaf607",
            "b1b0679208ed4067b69a9cb3b9970aeb",
            "9fcea3a58fc14c87ab848257fe44e1a4",
            "6ff05075bdaf446f8e4fa4b40e4614c0",
            "a14e8eaac4e74a619238af86a79415aa",
            "88a492cbbc3440ca86af756ad3c64c51",
            "89b9ac3f8743484992c139287933d128",
            "90480765ce524c49a28419b57d47cf06",
            "1bdc9c7b30ef4f0285cbcce5c2bcb37b",
            "1d379e1ae2164cda9cc41fcc3dc53bcd",
            "8c5ce11f3442453b9915eec1d2289d3b",
            "f6c2202177b34c6ca38815dd0c43bd5b",
            "2ce10d1cdea14c55aff8b280fc476a9d",
            "4c362dad328149e9be64571403a9cca9",
            "d7ab0638bc964b23aff867df2ee0ab19"
          ]
        },
        "id": "xIIeWliGDKCn",
        "outputId": "5dc76f8e-c921-46e7-e359-30516d664988"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efb5ba700c564ecf98cbf643d3fd03a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a14e8eaac4e74a619238af86a79415aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "config = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    fp16=True,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    #tokenizer=tokenizer,\n",
        "    args=config,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kZs06x8aUSZ"
      },
      "source": [
        "Lancement de l'entraînement du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "XQjv5DX7V7rN",
        "outputId": "8233f16a-699f-4175-9ded-44d5e9180834"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Casting fp32 inputs back to torch.float16 for flash-attn compatibility.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 1:16:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Similarity</th>\n",
              "      <th>Entropy</th>\n",
              "      <th>Num Tokens</th>\n",
              "      <th>Mean Token Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.430200</td>\n",
              "      <td>2.383633</td>\n",
              "      <td>0.442938</td>\n",
              "      <td>0.159910</td>\n",
              "      <td>0.397586</td>\n",
              "      <td>0.360544</td>\n",
              "      <td>1.012557</td>\n",
              "      <td>456000.000000</td>\n",
              "      <td>0.494393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.238600</td>\n",
              "      <td>2.189517</td>\n",
              "      <td>0.466294</td>\n",
              "      <td>0.183500</td>\n",
              "      <td>0.423799</td>\n",
              "      <td>0.392219</td>\n",
              "      <td>0.916236</td>\n",
              "      <td>912000.000000</td>\n",
              "      <td>0.519947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=2.3692479515075684, metrics={'train_runtime': 4570.2019, 'train_samples_per_second': 1.75, 'train_steps_per_second': 0.109, 'total_flos': 3.9052977242112e+16, 'train_loss': 2.3692479515075684, 'epoch': 2.0})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4UqfvSWex-l"
      },
      "source": [
        "Évaluation finale du modèle sur le jeu de données de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNw-fBiTmGgH"
      },
      "source": [
        "## Evaluation et génération de texte\n",
        "\n",
        "Nous allons maintenant évaluer les performances du modèle sur le jeu de données de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "ZyhS3bCzeCgo",
        "outputId": "1fa7ac51-29b0-451f-f114-61adb895e2b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:33]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "results = trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11jIgB-pf0Xs",
        "outputId": "4aaa7ed4-ffe1-4b1c-c526-1a37c747d7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultats de l'évaluation :\n",
            " {'eval_loss': 2.1895174980163574, 'eval_rouge1': 0.46629407424436614, 'eval_rouge2': 0.18350005886115567, 'eval_rougeL': 0.42379894033658566, 'eval_similarity': 0.3922194751762901, 'eval_runtime': 163.4913, 'eval_samples_per_second': 6.117, 'eval_steps_per_second': 6.117, 'eval_entropy': 0.916236186876893, 'eval_num_tokens': 912000.0, 'eval_mean_token_accuracy': 0.5199474306106567, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"Résultats de l'évaluation :\\n\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsx9ZIGXmjYj"
      },
      "source": [
        "Nous pouvons tester les capacités de génération du modèle à partir d'un prompt utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAScXNFoKkPM"
      },
      "outputs": [],
      "source": [
        "def generate_story(input_text, role):\n",
        "    results = []\n",
        "    input = input_text\n",
        "    prompt = f\" <|system|>role: {role}</s><|user|>prompt: {input}<|assistant|></s>\"\n",
        "    #output =dataset['story'][i]\n",
        "    tokenized_input = tokenizer(prompt, return_tensors=\"pt\", max_length=512, padding=True, truncation=True)\n",
        "    input_ids = tokenized_input[\"input_ids\"]\n",
        "    attention_mask = tokenized_input[\"attention_mask\"]\n",
        "    response = model.generate(\n",
        "        input_ids=input_ids.to(model.device),\n",
        "        attention_mask=attention_mask.to(model.device),\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.6,\n",
        "        top_k=70,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    response_text = tokenizer.decode(response[0])\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxaRG_4PMSgg",
        "outputId": "266df3ec-d3e8-43f8-cfea-05a3a2e0b58c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> <|system|>role: You are an assistant specialized in creative story writing, based ont the prompt given to you by the user. </s><|user|>prompt: You have witnessed the creation of humanity. Tell me the whole story of how humans were created.<|assistant|></s> The story of human creation is a complex and fascinating one, filled with mystery and wonder. Let us begin at the dawn of time, when the universe was but a swirling mass of cosmic dust and gas.\n",
            "\n",
            "In the heart of this cosmic storm, a brilliant star was born. This star, known as Sol, would one day become the sun that warms and lights our world. But for now, it was just a small, insignificant speck in the vast expanse of space.\n",
            "\n",
            "As Sol grew, it began to attract other celestial bodies. Among them was a small, rocky planet, which would one day become Earth. The planet was barren and lifeless, but it had the potential to support life.\n",
            "\n",
            "Millions of years passed, and Earth began to change. The planet's atmosphere thickened, and water began to accumulate on its surface. The first life forms emerged, simple organisms that could survive in the harsh conditions of the early Earth.\n",
            "\n",
            "Over time, these organisms evolved and diversified, adapting to their environment and developing new abilities. Some could move, others could photosynthesize, and some could even communicate with each other.\n",
            "\n",
            "But it was not until the emergence of Homo sapiens that humanity truly came into being. These early humans were intelligent and curious, and they quickly began to dominate the planet. They hunted and gathered, built shelters and tools, and formed complex societies.\n",
            "\n",
            "As they spread across the globe, they encountered other human species. Some of these species were closely related to Homo sapiens, while others were more distant cousins. But all of them shared a common ancestry, and they all traced their lineage back to the first life forms that emerged on Earth millions of years ago.\n",
            "\n",
            "And so, humanity was born. From simple organisms to intelligent beings, we have come a long way. But our journey is far from over, and we still have much to learn about ourselves and the world around us. The story of human creation is a tale of resilience, adaptation, and the indomitable spirit of life. It is a story that continues to unfold, and one that we will continue to write for as long as we exist.</s>\n"
          ]
        }
      ],
      "source": [
        "user_prompt = \"You have witnessed the creation of humanity. Tell me the whole story of how humans were created.\"\n",
        "role = \"You are an assistant specialized in creative story writing, based ont the prompt given to you by the user. \"\n",
        "model_response = generate_story(user_prompt, role)\n",
        "print(model_response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}